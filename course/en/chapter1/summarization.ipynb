{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W4uHn0tmj67T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: xxhash in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63pksGqFj67U"
      },
      "source": [
        "# Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "gLoSsYlmj67W",
        "outputId": "93f9d791-ef70-4fe2-c4d1-2d0d0ace05eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ahana\\OneDrive\\Desktop\\Hugging_face_LLM\\.venv\\Lib\\site-packages\\IPython\\core\\display.py:447: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yHnr5Dk2zCI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yHnr5Dk2zCI?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8NeAPmj67X"
      },
      "source": [
        "Summarization creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. Summarization can be:\n",
        "\n",
        "- Extractive: extract the most relevant information from a document.\n",
        "- Abstractive: generate new text that captures the most relevant information.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [T5](https://huggingface.co/t5-small) on the California state bill subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive summarization.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [Blenderbot](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot), [BlenderbotSmall](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot-small), [Encoder decoder](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/encoder-decoder), [FairSeq Machine-Translation](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fsmt), [GPTSAN-japanese](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptsan-japanese), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LongT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longt5), [M2M100](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/m2m_100), [Marian](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/marian), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mt5), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [NLLB](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb), [NLLB-MOE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb-moe), [Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus), [PEGASUS-X](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus_x), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/prophetnet), [SwitchTransformers](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/switch_transformers), [T5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/t5), [XLM-ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-prophetnet)\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate rouge_score\n",
        "```\n",
        "\n",
        "We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "raGHcaMDj67Y"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48dc439a68714f96b14dd1bef4c83d16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOsYNiXWj67Z"
      },
      "source": [
        "## Load BillSum dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QarSiCLj67Z"
      },
      "source": [
        "Start by loading the smaller California state bill subset of the BillSum dataset from the 🤗 Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uHGbHS5bj67a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TsWGPzcj67b"
      },
      "source": [
        "Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KIqYOkRmj67c"
      },
      "outputs": [],
      "source": [
        "billsum = billsum.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Xmpbe1j67c"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JWMjQEa4j67d",
        "outputId": "5449cd6b-c93c-4b20-e7dc-76042638897b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 27388 of the Government Code is amended to read:\\n27388.\\n(a) (1) In addition to any other recording fees specified in this code, upon the adoption of a resolution by the county board of supervisors, a fee of up to ten dollars ($10) shall be paid at the time of recording of every real estate instrument, paper, or notice required or permitted by law to be recorded within that county, except those expressly exempted from payment of recording fees and except as provided in paragraph (2). For purposes of this section, “real estate instrument” means a deed of trust, an assignment of deed of trust, an amended deed of trust, an abstract of judgment, an affidavit, an assignment of rents, an assignment of a lease, a construction trust deed, covenants, conditions, and restrictions (CC&Rs), a declaration of homestead, an easement, a lease, a lien, a lot line adjustment, a mechanics lien, a modification for deed of trust, a notice of completion, a quitclaim deed, a subordination agreement, a release, a reconveyance, a request for notice, a notice of default, a substitution of trustee, a notice of trustee sale, a trustee’s deed upon sale, or a notice of rescission of declaration of default, or any Uniform Commercial Code amendment, assignment, continuation, statement, or termination. The fees, after deduction of any actual and necessary administrative costs incurred by the county recorder in carrying out this section, shall be paid quarterly to the county auditor or director of finance, to be placed in the Real Estate Fraud Prosecution Trust Fund. The amount deducted for administrative costs shall not exceed 10 percent of the fees paid pursuant to this section.\\n(2) The fee imposed by paragraph (1) shall not apply to any real estate instrument, paper, or notice if any of the following apply:\\n(A) The real estate instrument, paper, or notice is accompanied by a declaration stating that the transfer is subject to a documentary transfer tax pursuant to Section 11911 of the Revenue and Taxation Code.\\n(B) The real estate instrument, paper, or notice is recorded concurrently with a document subject to a documentary transfer tax pursuant to Section 11911 of the Revenue and Taxation Code.\\n(C) The real estate instrument, paper, or notice is presented for recording within the same business day as, and is related to the recording of, a document subject to a documentary transfer tax pursuant to Section 11911 of the Revenue and Taxation Code. A real estate instrument, paper, or notice that is exempt under this subparagraph shall be accompanied by a statement that includes both of the following:\\n(i) A statement that the real estate instrument, paper, or notice is exempt from the fee imposed under paragraph (1).\\n(ii) A statement of the recording date and the recorder identification number or book and page of the previously recorded document.\\n(b) Money placed in the Real Estate Fraud Prosecution Trust Fund shall be expended to fund programs to enhance the capacity of local police and prosecutors to deter, investigate, and prosecute real estate fraud crimes. After deduction of the actual and necessary administrative costs referred to in subdivision (a), 60 percent of the funds shall be distributed to district attorneys subject to review pursuant to subdivision (d), and 40 percent of the funds shall be distributed to local law enforcement agencies within the county in accordance with subdivision (c). In those counties where the investigation of real estate fraud is done exclusively by the district attorney, after deduction of the actual and necessary administrative costs referred to in subdivision (a), 100 percent of the funds shall be distributed to the district attorney, subject to review pursuant to subdivision (d). A portion of the funds may be directly allocated to the county recorder to support county recorder fraud prevention programs, including, but not limited to, the fraud prevention program provided for in Section 27297.7. Prior to establishing or increasing fees pursuant to this section, the board of supervisors may consider support for county recorder fraud prevention programs. The funds so distributed shall be expended for the exclusive purpose of deterring, investigating, and prosecuting real estate fraud crimes.\\n(c) The county auditor or director of finance shall distribute funds in the Real Estate Fraud Prosecution Trust Fund to eligible law enforcement agencies within the county pursuant to subdivision (b), as determined by a Real Estate Fraud Prosecution Trust Fund Committee composed of the district attorney, the county chief administrative officer, the chief officer responsible for consumer protection within the county, and the chief law enforcement officer of one law enforcement agency receiving funding from the Real Estate Fraud Prosecution Trust Fund, the latter being selected by a majority of the other three members of the committee. The chief law enforcement officer shall be a nonvoting member of the committee and shall serve a one-year term, which may be renewed. Members may appoint representatives of their offices to serve on the committee. If a county lacks a chief officer responsible for consumer protection, the county board of supervisors may appoint an appropriate representative to serve on the committee. The committee shall establish and publish deadlines and written procedures for local law enforcement agencies within the county to apply for the use of funds and shall review applications and make determinations by majority vote as to the award of funds using the following criteria:\\n(1) Each law enforcement agency that seeks funds shall submit a written application to the committee setting forth in detail the agency’s proposed use of the funds.\\n(2) In order to qualify for receipt of funds, each law enforcement agency submitting an application shall provide written evidence that the agency either:\\n(A) Has a unit, division, or section devoted to the investigation or prosecution of real estate fraud, or both, and the unit, division, or section has been in existence for at least one year prior to the application date.\\n(B) Has on a regular basis, during the three years immediately preceding the application date, accepted for investigation or prosecution, or both, and assigned to specific persons employed by the agency, cases of suspected real estate fraud, and actively investigated and prosecuted those cases.\\n(3) The committee’s determination to award funds to a law enforcement agency shall be based on, but not be limited to, (A) the number of real estate fraud cases filed in the prior year; (B) the number of real estate fraud cases investigated in the prior year; (C) the number of victims involved in the cases filed; and (D) the total aggregated monetary loss suffered by victims, including individuals, associations, institutions, or corporations, as a result of the real estate fraud cases filed, and those under active investigation by that law enforcement agency.\\n(4) Each law enforcement agency that, pursuant to this section, has been awarded funds in the previous year, upon reapplication for funds to the committee in each successive year, in addition to any information the committee may require in paragraph (3), shall be required to submit a detailed accounting of funds received and expended in the prior year. The accounting shall include (A) the amount of funds received and expended; (B) the uses to which those 1695.1 of the Civil Code. Case filing decisions continue to be at the discretion of the prosecutor.\\n(g) A district attorney’s office or a local enforcement agency that has undertaken investigations and prosecutions that will continue into a subsequent program year may receive nonexpended funds from the previous fiscal year subsequent to the annual submission of information detailing the accounting of funds received and expended in the prior year.\\n(h) No money collected pursuant to this section shall be expended to offset a reduction in any other source of funds. Funds from the Real Estate Fraud Prosecution Trust Fund shall be used only in connection with criminal investigations or prosecutions involving recorded real estate documents.',\n",
              " 'summary': 'Existing law authorizes the board of supervisors to adopt, by resolution, a fee of up to $10 for each recording of a real estate instrument, paper, or notice required or permitted by law to be recorded, except as specified. Existing law defines the term “real estate instrument” to exclude a deed, instrument, or writing recorded in connection with a transfer subject to a documentary transfer tax.\\nThis bill would recast this latter exclusion from a “real estate instrument” as a statement that the above-described fee does not apply to any real estate instrument, paper, or notice accompanied by a declaration stating that the transfer is subject to a documentary transfer tax, is recorded concurrently with a transfer subject to a documentary transfer tax, or is presented for recording within the same business day as, and is related to the recording of, a transfer subject to a documentary transfer tax.',\n",
              " 'title': 'An act to amend Section 27388 of the Government Code, relating to local government.'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "billsum[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXBmpVWzj67d"
      },
      "source": [
        "There are two fields that you'll want to use:\n",
        "\n",
        "- `text`: the text of the bill which'll be the input to the model.\n",
        "- `summary`: a condensed version of `text` which'll be the model target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYs69UzGj67d"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwXblzM4j67e"
      },
      "source": [
        "The next step is to load a T5 tokenizer to process `text` and `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4n_Rv1obj67e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUKis9foj67e"
      },
      "source": [
        "The preprocessing function you want to create needs to:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "2. Use the keyword `text_target` argument when tokenizing labels.\n",
        "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_D91337Kj67e"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dJw9XLoj67e"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use 🤗 Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OkRFY4pqj67f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7e65a5825b14b71b3aef3d7da67100f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6509688aeeab4e3ab3b4ebe8d4e48309",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSuJtzKjj67f"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8aA-8Fi6j67f"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZrLxWsWj67f"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gg1fkqj67f"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: rouge_score in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from rouge_score) (2.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\ahana\\onedrive\\desktop\\hugging_face_llm\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install absl-py rouge_score nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kxu54RtRj67f"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKiCrKhsj67f"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the ROUGE metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o75TE-gmj67f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WdKRqpJj67f"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J6aXaHRj67g"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ58XGmMj67g"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1EvB7p0sj67g"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.memory_allocated())\n",
        "print(torch.cuda.memory_reserved())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDhkbjRnj67g"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the ROUGE metric and save the training checkpoint.\n",
        "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZHqohWX9j67g"
      },
      "outputs": [],
      "source": [
        "# training_args = Seq2SeqTrainingArguments(\n",
        "#     output_dir=\"my_awesome_billsum_model\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=16,\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     weight_decay=0.01,\n",
        "#     save_total_limit=3,\n",
        "#     num_train_epochs=4,\n",
        "#     predict_with_generate=True,\n",
        "#     fp16=True,\n",
        "#     push_to_hub=True,\n",
        "# )\n",
        "\n",
        "# trainer = Seq2SeqTrainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_billsum[\"train\"],\n",
        "#     eval_dataset=tokenized_billsum[\"test\"],\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_collator=data_collator,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ahana\\AppData\\Local\\Temp\\ipykernel_11184\\3707066336.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ],
      "source": [
        "#Optimized version\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"my_awesome_billsum_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,  # Reduced batch size\n",
        "    per_device_eval_batch_size=8,   # Reduced batch size\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,  # Keep only the most recent checkpoint\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    gradient_accumulation_steps=2,  # Use gradient accumulation\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_billsum[\"train\"],\n",
        "    eval_dataset=tokenized_billsum[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnerb_5sj67g"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kPv3N1Lpj67g"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3f9f25d387246ef88d78d0b1329443a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a192c815982542d28010236a3b628127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c9449a118ea41e79e4cee5b404be729",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/AhChat/my_awesome_billsum_model/commit/b65ca5cfe7cca321695041539e4f3c960c766564', commit_message='End of training', commit_description='', oid='b65ca5cfe7cca321695041539e4f3c960c766564', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AhChat/my_awesome_billsum_model', endpoint='https://huggingface.co', repo_type='model', repo_id='AhChat/my_awesome_billsum_model'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMPlcMsXj67g"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for summarization, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-33oN-sj67g"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulK0MDmWj67h"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Come up with some text you'd like to summarize. For T5, you need to prefix your input depending on the task you're working on. For summarization you should prefix your input as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "emXCGNioj67h"
      },
      "outputs": [],
      "source": [
        "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrdTkLTjj67m"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for summarization with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JMHJ6NL-j67n",
        "outputId": "112529de-3770-4636-c390-b36a976ea157"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838579055b6e41c9aa86cfdee1f9b641",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ahana\\OneDrive\\Desktop\\Hugging_face_LLM\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ahana\\.cache\\huggingface\\hub\\models--AhChat--my_awesome_billsum_model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fb5bfbecdcb4fada2e67f4bf530d6ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfb9900cc20e413e86baa369469f39d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/149 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f72312f9b6094bceb2cf9c8104faf9a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/21.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46c1331cf3c4b44a673f25fcf3a8459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71de097b0d7482ba6626ac752f66c33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.67k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Your max_length is set to 200, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': \"the Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs . it's the most aggressive action on tackling the climate crisis in history . no one making under $400,000 per year will pay a penny more in taxes .\"}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"AhChat/my_awesome_billsum_model\")\n",
        "summarizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmMNclh5j67n"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "\n",
        "Tokenize the text and return the `input_ids` as PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2nt8c0hqj67n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"AhChat/my_awesome_billsum_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvVlxdqQj67n"
      },
      "source": [
        "Use the [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to create the summarization. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/text_generation) API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TPTzEXTij67n"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"AhChat/my_awesome_billsum_model\")\n",
        "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4d6EpGqj67n"
      },
      "source": [
        "Decode the generated token ids back into text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4AyK4nsPj67n",
        "outputId": "e4e56e00-118e-4983-84bc-3824e68c6c22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
